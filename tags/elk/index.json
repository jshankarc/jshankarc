[{"content":"Data Science and Sports Analytics are my passions. My name is Jayashankar and I have been working in the data science field doing sports analytics for the last 5 years. I have held data science positions in companies ranging from startups to fortune 100 organizations. I transitioned into data science from a business and consulting background. When I was first starting out on my data science journey I was extremely lost; there were very few resources for me to learn about this field from. I decided to start making youtube videos to share my experiences and to hopefully help others get break into the data science and sports analytics fields.\n","description":"Hugo, the world’s fastest framework for building websites","id":0,"section":"","tags":null,"title":"About","uri":"https://jshankarc.github.io/about/"},{"content":"Note: This post is originally from my https://jshankarc.blogspot.com and now moved it to github pages.\nIntroduction The main motivation for developing this POC is to monitor CPU and memory usage of the 45 containers and the current tools or technology which were used does not provide out of the box features to enable it.\nI found ELK stack is the quickest and possible solution to solve this issue.\nElastic Stack is powerful, interesting and easy to get started with some basic knowledge about the individual components and their integration patterns.\nThe scope is to understand the configurations to enable monitoring of containers which exposes Jolokia REST servers using ELK stack along with Metricbeat.\nBefore I start the configuration of individual components, let me brief you about my setup.\nElasticsearch It is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected.\nKibana It is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices. You can easily perform advanced data analysis and visualize your data in a variety of charts, tables, and maps.\nLogstash It is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.” (Ours is Elasticsearch, naturally.)\nMetricbeat It collects metrics from your systems and services. From CPU to memory, Redis to NGINX, and much more, Metricbeat is a lightweight way to send system and service statistics.\nI am using JBoss Fuse container which exposes Jolokia endpoints.\nThis configuration is not specific to JBoss fuse container monitoring instead consider this as a walkthrough to enable Jolokia monitoring.\nJolokia is a standard and it is common wherever it is extended or used.\nst=\u0026gt;start: Start|past e=\u0026gt;end: End|future met=\u0026gt;operation: Metricbeat log=\u0026gt;operation: Logstash els=\u0026gt;operation: Elasticsearch kib=\u0026gt;operation: Kibana st-\u0026gt;met(right)-\u0026gt;log(right)-\u0026gt;els(right)-\u0026gt;kib-\u0026gt;e MetricBeat: metricbeat.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #composite-services - module: jolokia metricsets: [\u0026#34;jmx\u0026#34;] period: 1s hosts: [\u0026#34;http://{{ip}}:{{port}}\u0026#34;] #cannot find the documentation - user and pass username: user password: pass namespace: \u0026#34;composite-services-n1\u0026#34; path: \u0026#34;/jolokia/?ignoreErrors=true\u0026amp;canonicalNaming=false\u0026#34; jmx.mappings: - mbean: \u0026#39;java.lang:type=Memory\u0026#39; attributes: - attr: HeapMemoryUsage field: jvm.memory.heap_memory_usage - mbean: \u0026#39;java.lang:type=OperatingSystem\u0026#39; attributes: - attr: ProcessCpuLoad field: jvm.process.cpu.load jmx.application: jmx.instance:   Logstash: metricbeat-config.config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #composite-services input { beats { port =\u0026gt; 5044 } } filter { if [metricset][namespace] == \u0026#34;composite-services-n1\u0026#34; { ruby { code =\u0026gt; \u0026#39;event.set(\u0026#34;cpu.usage\u0026#34;, event.get(\u0026#34;[jolokia][composite-services-n1][jvm][process][cpu][load]\u0026#34;) * 100)\u0026#39; // process in % calculation } } } output { stdout { codec =\u0026gt; rubydebug } elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] } }   Configuration is completed and now ready to start individual components. 1 2 3 4 5 6 7 8 9 10 11  elasticsearch-6.0.0\\bin\u0026gt;elasticsearch logstash-6.0.0\\bin\u0026gt;logstash -f metricbeat-config.config metricbeat-6.0.0-windows-x86_64\u0026gt;metricbeat modules disable system metricbeat-6.0.0-windows-x86_64\u0026gt;metricbeat modules enable jolokia metricbeat-6.0.0-windows-x86_64\u0026gt;metricbeat -e -c metricbeat.yml kibana-6.0.0-windows-x86_64\\bin\u0026gt;kibana.bat   User link: http://localhost:5601/\nSelect the logstash-* as the indexer\nUnder Visualize, click on \u0026ldquo;Create new Visualization\u0026rdquo; choose Line Chart under Basic Charts.\nChoose\nIn From a New Search, Select Index, select logstash-*\nUnder Buckets\n X-Axis  Aggregation as Data Histogram Field as @timestamp Interval as Second   Y-Axis (1)  Aggregation as Max Field as jolokia.composite-services-n1.jvm.memory.heap_memory_usage.committed Custom Label as committed   Y-Axis (2)  Aggregation as Max Field as jolokia.composite-services-n1.jvm.memory.heap_memory_usage.used Custom Label as used   Y-Axis (3)  Aggregation as Max Field as jolokia.composite-services-n1.jvm.memory.heap_memory_usage.init Custom Label as init    Save as \u0026ldquo;composite-services\u0026rdquo;\nIn Dashboard, search for \u0026ldquo;composite-services\u0026rdquo; and again save it.\nA sample Dashboard screenshot is attached for the reference.\n","description":"A ELK stack integration to collect log and Visualize","id":1,"section":"posts","tags":["ELK","Elastic Search"],"title":"Collect, Analyze and Visualize Jolokia Metrics Using Elastic Stack","uri":"https://jshankarc.github.io/posts/collect-analyze-visualize-jolokia-metrics-using-elk/"},{"content":"Note: This post is originally from my https://jshankarc.blogspot.com and now moved it to github pages.\nIntroduction It was long time plan to write a blog on Apache Camel and finally came up with an idea that felt interesting to me and hope you will enjoy it.\nThe solution to integrate camel with Jboss Fuse, Syslog and Elastic Stack eventually becomes a design.\nI would like to start by stating a problem and then discuss about solving it step by step.\nProblem: I had an use case where the incoming and outgoing messages are used for tracking and analysis purpose. There was already a process in place for doing same by using wiretap endpoint in camel to route message asynchronously for storing the messages as a file.\nExample for Wiretap config:\nfile:/apps/esb/log/service /Rq/?fileName=$simple{date:now:yyyy}/$simple{date:now:MM}/$simple{date:now:dd}/$simple{RequestID}.xml  However, this created a problem whenever high load hits the server, it eventually resulted in high usage of memory in the container. There were 170 and more services which was deployed in production and changing those services would be risk to follow up on testing and re-deploying. Luckily these properties were externalized and then I figured out a solution to just change the configuration.\nSo in the post, I will be explaining about the solution to the problem stated by leveraging Apache camel custom component feature and extending further to build analytics.\nSolution We can split our problems into two phases. In the first phase, I will discuss on fixing the actual problem of logging the messages in files and next, enhancing it to create proper structured data, centralizing the messages and visualize for analytics.\nRefer code from github repository.\nStep 1 Just like any other producer endpoints such as http, log, file etc, we are going to create our own camel custom producer endpoint, in which the exchange messages are pushed to the component. In our case, LogManagerProducer class will receive the exchange. MessageType is the POJO class were we will be mapping all the necessary parameters from the exchange, then convert into JSON payload. This data format will remove new line characters and in later stages parsing using logstash becomes easier.\n1 2 3 4 5 6 7 8 9 10 11  MessageType messageType = new MessageType(); messageType.setRequestId(exchange.getProperty(\u0026#34;transactionId\u0026#34;,String.class)); messageType.setMessageType(endpoint.getMessageType()); messageType.setServiceName(endpoint.getServiceName()); messageType.setLoggingTime(endpoint.getLogTime()); messageType.setData(exchange.getIn().getBody(String.class)); Gson gson = new GsonBuilder().disableHtmlEscaping().create(); SyslogMessageTransmitter transmitter = new SyslogMessageTransmitter(); transmitter.send(gson.toJson(messageType));   Below is the code snippet from the SyslogMessageTransmitter class to configure Syslog message. Refer this link to understand syslog protocol features. The implementation logic is taken from CloudBees-community which is simple to use. Most over, I am not interested in rewriting code which is already available for doing the same task.\n1 2 3 4 5 6 7 8 9 10  TcpSyslogMessageSender messageSender = new TcpSyslogMessageSender(); messageSender.setDefaultFacility(Facility.LOCAL0); messageSender.setDefaultSeverity(Severity.INFORMATIONAL); //to run the Junit, change the Ip address and port of the syslog server messageSender.setSyslogServerHostname(\u0026#34;localhost\u0026#34;); messageSender.setSyslogServerPort(port); messageSender.setMessageFormat(MessageFormat.RFC_3164); messageSender.setSsl(false); messageSender.setMaxRetryCount(3); messageSender.sendMessage(message);   Here, send method is responsible for pushing message via TCP channel.\nRsyslog configuration Change the Rsyslog server configuration as below and restart the service\n/etc/rsyslog.config\n1 2 3 4 5 6 7  Uncomment $ModLoad imtcp $InputTCPServerRun 514 add # ESB Transaction Logs local0.info /var/log/esb-transaction-logs.log   However, I have also create sample project to communicate with logmanager component and you find that in repo as camel-log-demo.\nWith the above details, we are now ready to deploy two projects into JBoss Fuse container and messages can be seen in /var/log/esb-transaction-logs.log file.\nIn this phase, we looked at how to develop camel custom component and configure Rsyslog through which message are logged to a single file. Rsyslog has built-in features to use log rotation policies. However, in the next phase, we can extend current capability to centralize the messages from different servers.\nPhase 2: We can see the design solution of our project, Elastic stack is used to visualize the messages and acts as a centralized server to manage logs and generate reports in our case.\nIn the large enterprise, multiple nodes are used for load balancing the messages across servers. The main problem we will be facing in such cases are to identify which server has served particular incoming request. Typically, we will log into each server checking log messages. However, this will eventually become a painful task.\nFor instance, the request message can contain an unique ID and it will help to trace certain messages and its execution but still we have not arrived at complete solution to centralize logs.\nInstall, Elastic stack on log server and it has independent components such as Logstash, Elasticsearch and Kibana.\nLogstash is used to parse the message and configuration is listed below:\nFile Name: indexpattern.config\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  input { tcp { type =\u0026gt; \u0026#39;esbtranslog\u0026#39; port =\u0026gt; 514 } } filter { if [type] == \u0026#34;esbtranslog\u0026#34; { grok { match =\u0026gt; { message =\u0026gt; \u0026#34;\u0026lt;%{WORD:ignore}\\\u0026gt;%{SYSLOGTIMESTAMP:time} %{SYSLOGHOST:clientip} -: %{GREEDYDATA:request}\u0026#34; } } json { source =\u0026gt; \u0026#34;request\u0026#34; target =\u0026gt; \u0026#34;log\u0026#34; remove_field=\u0026gt;[\u0026#34;request\u0026#34;] } } } output { if [type] == \u0026#34;esbtranslog\u0026#34; { elasticsearch { hosts =\u0026gt; \u0026#39;http://localhost:9200\u0026#39; index =\u0026gt; \u0026#39;logstash-local-%{+YYYY.MM.dd}\u0026#39; document_type =\u0026gt; \u0026#39;%{type}\u0026#39; } } }   Add below configuration for Rsyslog:\n1  /etc/rsyslog.d/pushtoserver.conf   Now, lets integrate all the three configured components.\n1 2 3  Logstash/bin\u0026gt; logstash -f indexpattern.config Elasticsearch/bin\u0026gt; elasticsearch Kibana/bin\u0026gt; kibana   Copy two bundles camel-log-demo and camel-logmanager-component into Jboss-fuse/deploy directory.\nFinally, open http://ip-address:5601 and you can see log message as shown below:\nTo conclude, I have just discussed about the high level design with some coding snippets. The main objective was to convey the design for logging messages since in this fast paced growing technologies, logging has not got standard.\nThis is my first post, I am very excited to share with you.\nHope it will be helpful.\n","description":"A Server Log Managing Architecture and Implementation","id":2,"section":"posts","tags":["ELK","Elastic Search"],"title":"Message Tracking Solution using Apache Camel, JBoss Fuse, Elastic Stack and Syslog","uri":"https://jshankarc.github.io/posts/log-msg-tracking-solution/"}]